{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `datascience` Library Demo Notebook\n",
    "\n",
    "_Notebook created by Chris Pyles_\n",
    "\n",
    "This notebook is intended to give you some basic information on manipulating rectangular data using the `datascience` library. The `datascience` library is a module for Python developed at UC Berkeley and which is used in the course Data 8: Foundations of Data Science. This notebook covers basic table operations using this library.\n",
    "\n",
    "<!--\n",
    "\n",
    "**Table of Contents**\n",
    "1. [Dependences](#Dependencies)\n",
    "2. [Loading Data](#Loading-Data)\n",
    "3. [Moving Between `pandas` and `datascience`](#Moving-Between-pandas-and-datascience)\n",
    "4. [Rows and Columns](#Rows-and-Columns)\n",
    "5. [Accessing Vaues](#Accessing-Values)\n",
    "6. [Missing Values](#Missing-Values)\n",
    "7. [Descriptive Statistics](#Descriptive-Statistics)\n",
    "8. [Grouping](#Grouping)\n",
    "9. [Manipulating Values](#Manipulating-Values)\n",
    "10. [Exporting Figures](#Exporting-Figures)\n",
    "11. [Exporting Data](#Exporting-Data)\n",
    "12. [Conclusion](#Conclusion)\n",
    "\n",
    "-->\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "In the cell below we load the dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "The method that `datascience` provides for reading in data defaults to reading CSV files. The function, `Table.read_table()`, takes as its argument a relative path to the data file. In the cell below, we load the datasets we will be using for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips = Table.read_table('data/trips.csv')\n",
    "stations = Table.read_table('data/stations.csv')\n",
    "trips.show(5)\n",
    "stations.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `Table.show()` method above to display the first 5 lines of each table. This method defaults to all rows, so calling `trips.show()` would have displayed all 354,152 rows of that table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have files that use other delimeters, you can pass the `sep` argument of `pd.read_csv()` to `Table.read_table()` and the file will be read in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Table.read_table(\"data/trips.tsv\", sep=\"\\t\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have data formatted in ways other than delimited files, these need to be loaded into `pandas` first before being transferred to `datascience`. An example call is given below.\n",
    "\n",
    "```python\n",
    "# load data into pandas\n",
    "trips_df = pd.read_json(\"data/trips.json\")\n",
    "\n",
    "# transfer to datascience\n",
    "trips_tbl = Table.from_df(trips_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Between `pandas` and `datascience`\n",
    "\n",
    "As noted above, it is possible to transfer your data between `pandas` and `datascience`. The functions to do this are provided in the `datascience` library; `Table.from_df()` takes a DataFrame and returns a Table and `Table.to_df()` turns the Table into a DataFrame.\n",
    "\n",
    "```python\n",
    "# pandas to datascience\n",
    "tbl = Table.from_df(df)\n",
    "\n",
    "# datascience to pandas\n",
    "df = tbl.to_df()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rows and Columns \n",
    "\n",
    "To get row and column counts, the `datascience` library provides the `num_rows` and `num_columns` attributes, which are self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.num_rows, trips.num_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the labels of the columns, `datascience` has `labels`, which is a tuple containing the column labels in numerical index order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add columns to a table, you pass a single label and set of values to `.with_column()` or a list of labels and pairs to `.with_columns()` (both shown below). **These functions do not edit the original table, so these modifications can only be saved by assigning them to the name of the table or a new variable name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding a single column\n",
    "some_random_numbers = np.random.uniform(0, 10, trips.num_rows)\n",
    "trips.with_column(\"Random Numbers\", some_random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding multiple columns\n",
    "some_more_random_numbers = np.random.normal(0, 10, trips.num_rows)\n",
    "trips.with_columns(\n",
    "    \"Random Numbers\", some_random_numbers,\n",
    "    \"More Random Numbers\", some_more_random_numbers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the `.with_columns()` call, the column labels and values alternate; that is, the call should have the form\n",
    "\n",
    "```python\n",
    "tbl.with_columns(\n",
    "    \"Label 1\", values_1,\n",
    "    \"Label 2\", values_2,\n",
    "    \"Label 3\", values_3,\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "It is also important that the values argument(s) have the same number of rows as the table they are being added to. A single value entered as this argument will be broadcast to the entire table, but any length besides 1 or the number of rows in the table will throw an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to change the labels of columns using the `.relabeled()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.relabeled(\"Duration\", \"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Values\n",
    "\n",
    "For all non-continuous variables, it is usually important to understand the possible values of the variable; that is, to know the variable's _unique_ values. While `datascience` does not have a built-in method, it is a simple thing to export a column as an array and pass it to `np.unique`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.unique(trips.column('Start Date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `datascience` library provides the `.where()` method to filter rows, which uses a column name and a predicate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.where(\"Duration\", lambda x: x < 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library also provides the `are` class to create predicate functions. Each method of this class returns a boolean function that can be called on a value. For example, if we wanted a function that checked whether or not a value is greater than or equal to 1000, we could use the call below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "are.above_or_equal_to(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass these `are` objects to the `.where()` method to use as predicate functions. This is how students in Data 8 are taught to filter rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.where(\"Duration\", are.below(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full list of predicate functions, see the [`datascience.predicates` documentation](http://data8.org/datascience/predicates.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sort the rows of a table, use the `.sort()` method. It defaults to ascending, so to get values in `descending` order the `descending` argument must be set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.sort(\"Duration\", descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "The `datascience` library does not currently have the functionality to support working with missing values, although it is possible to transfer your data to `pandas` and use that library's tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is possible to combine row filtering with NumPy functions (or `pandas` ones) to do some simple filtering. As an example, if we wanted to filter out rows with missing values in a specific column, we could define our own predicate function as below and then use the `.where()` method to filter rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_nan = lambda x: not pd.isna(x)\n",
    "\n",
    "trips.where('End Terminal', not_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to filter rows with missing values in _any_ column, we could iterate through the labels in `Table.labels`, using the `.where()` method to filter on each pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for label in trips.labels:\n",
    "    trips = trips.where(label, not_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "In order to understand the distribution of your numerical data, it can be very useful to look at descriptive statistics of the values. The `datascience` library allows you to compute statistics on each column of your table, but it requires you to specify which operations you want to run and it does not filter out non-numerical columns.\n",
    "\n",
    "To use the `datascience` library to get descriptive statistics, use the `.stats()` method; this requires you to specify which statistics you want to use to aggregate each column, which you do by passing a list of functions as the `ops` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# datascience\n",
    "first_quartile = lambda x: np.quantile(x, 0.25)\n",
    "third_quartile = lambda x: np.quantile(x, 0.75)\n",
    "trips.stats(ops = [min, max, np.mean, np.std, first_quartile, third_quartile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default behavior of the `.stats()` method is to show the minimum, maximum, median, and sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "In the `datascience` library, you can group by a column with the `.group()` method; this defaults to counts, but you can pass an optional second argument with an aggregator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.group('Start Station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you pass an aggregator function, each column is aggregated by that function in the specified groups. This means that the new table will have the same number of columns as the original, unlike the call _without_ an aggregator function. As an example of an aggregator function, we could pass `np.median()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.group(\"Start Station\", np.median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a pivot table, use the `.pivot()` method. The first argument indicates the column labels, the second the rows, and the third the values that go into each entry. If there are more than one value to go into the cells, it is also possible to pass an aggregator function. The cell below shows a table where each column is a starting station, each row is an ending station, and each value is the mean of the durations for that starting and ending station pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.pivot(\"Start Station\", \"End Station\", \"Duration\", np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Tables\n",
    "\n",
    "The `datascience` library allows you to join tables using its `.join()` method. This method performs an _inner_ join, which means that the rows are only those whose values in the join column(s) appear in _both_ tables. \n",
    "\n",
    "The call below joins the `trips` table with the second through fourth columns of the `stations` table, left on `\"Start Station\"` and right on `\"name\"`. This means that the result table will have two new columns, `\"lat\"` and `\"long\"`, indicating the latitude and longitude of the starting station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.join(\"Start Station\", stations.select(1, 2, 3), \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform other types of joins, the tables would need to be passed to `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Values\n",
    "\n",
    "The most common way to manipulate a data set is to apply a predefined function on each element of a column. To accomplish this in `datascience`, we utilize the `.apply()` method, which takes as its arguments first a function to apply and then the column index or label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "square = lambda x: x**2\n",
    "\n",
    "sqaured_durations = trips.apply(square, \"Duration\")\n",
    "trips = trips.with_column(\"Duration^2\", sqaured_durations)\n",
    "trips.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograding with OkPy\n",
    "\n",
    "UC Berkeley's Python courses use an autograder called [okpy](https://okpy.org). The package has an easy Jupyter Notebook integration, which is why it is the autograder infrastructure for so many Berkeley courses. Using this autograder requires writing tests, similar to doctests, that will be run in the local environment when you tell the autograder to check the notebook. These can be divided up into multiple sections, and are recorded in Python files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing OkPy Tests\n",
    "\n",
    "Okpy tests are written in \"ok format\"; this means that they are stored in your Python file as the variable `test`, which is a dictionary of information about the specific test. Each Python file is its own test. In the table below, the keys of the dictionary that are needed are described.\n",
    "\n",
    "| Key | Type | Description |\n",
    "|-----|-----|-----|\n",
    "| `\"name\"` | `str` | the name of the question |\n",
    "| `\"points\"` | `int`, `float` | the point value of the question |\n",
    "| `\"suites\"` | `list` | list of dictionaries with the code for the test, with some other attributes |\n",
    "\n",
    "The `\"suites\"` key should have a value that is a list of dictionaries, each of which has the following attributes:\n",
    "\n",
    "| Key | Type | Description |\n",
    "|-----|-----|-----|\n",
    "| `\"cases\"` | `list` | list of dictionaries with the code for each test |\n",
    "| `\"scored\"` | `bool` | whether or not the test is scored |\n",
    "| `\"setup\"` | `str` | setup code to run before the cases |\n",
    "| `\"teardown\"` | `str` | code to run after the cases |\n",
    "| `\"type\"` | `str` | type of test, usually set to `\"doctest\"` |\n",
    "\n",
    "Each test is divided into suites, which are in turn divided into cases. This is useful in CS courses, but a featured which is often not used in Data 8. For most of the tests you write, it is likely that `test[\"suites\"]` and `test[\"suites\"][0][\"cases\"]` will have length 1.\n",
    "\n",
    "As an example of an ok test file, consider the one below.\n",
    "\n",
    "```python\n",
    "test = {\n",
    "    \"name\": \"Question 1\",\n",
    "    \"points\": 1,\n",
    "    \"suites\": [\n",
    "        {\n",
    "            \"cases\": [\n",
    "                {\n",
    "                    \"code\": r\"\"\"\n",
    "                    >>> the_answer\n",
    "                    42\n",
    "                    >>> np.isclose(42, the_answer)\n",
    "                    True\n",
    "                    \"\"\",\n",
    "                    \"hidden\": False,\n",
    "                    \"locked\": False\n",
    "                }\n",
    "            ],\n",
    "            \"scored\": True,\n",
    "            \"setup\": \"\",\n",
    "            \"teardown\": \"\",\n",
    "            \"type\": doctest\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "As you can see, we check in this test that `the_answer` is an integer with value 42. If we hadn't had numpy imported in the notebook environment, then we would've needed to change the `\"setup\"` value to include that:\n",
    "\n",
    "```python\n",
    "\"setup\": r\"\"\"\n",
    ">>> import numpy as np\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Also note that the strings with code in them are all `r` strings. This is important for the interpreter.\n",
    "\n",
    "**Some things to keep in mind when writing tests:**\n",
    "* Rows and elements often get shuffled around due to student explorations. For this reason, it is often good to avoid indexing by numbers unless you are _100% certain_ that this won't happen.\n",
    "* Rounding errors occur, so use functions like `np.isclose` instead of tests for direct equality.\n",
    "* Write exhaustive tests but don't try to verify that each cell matches. In most cases, either every answer will be off or none of them well; figure out how to exploit this in your tests and writing them will be a lot easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage in Notebooks\n",
    "\n",
    "To initialize the autograder, you first need to import from the client package the `Notebook` object. The `client` package is set up when you install okpy on your JupyterHub. To initialize the autograder, create an instance of the `Notebook` class; by convention, we save this instance as `ok`. The `Notebook` initializer requires 1 argument, the relative path to your [ok configuration file](https://okpy.github.io/documentation/client.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from client.api.notebook import Notebook\n",
    "ok = Notebook(\"demo.ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the okpy website to collect submissions, then you would also have put the following line in the cell above:\n",
    "\n",
    "```python\n",
    "_ = ok.auth(inline=True)\n",
    "```\n",
    "\n",
    "This would direct your students to the okpy site to log in and give them an authentication key for the notebook.\n",
    "\n",
    "To run autograder tests, we use the `ok.grade()` function; it takes a single argument, the identifier of the tests you're trying to run. As an example, assign `the_answer` below to the value `42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "the_answer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = ok.grade(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I have stored the test cases in `tests/q1.py`, the autograder will go there and run the tests to make sure that you pass. (The location of the tests is set up in the ok config file.)\n",
    "\n",
    "To make life easier for the students, we often include the cell below which will allow them to run all of the ok tests at once to verify that all of their code is working.\n",
    "\n",
    "```python\n",
    "# For your convenience, you can run this cell to run all the tests at once!\n",
    "import os\n",
    "_ = [ok.grade(q[:-3]) for q in os.listdir(\"tests\") if q.startswith('q')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit work to the okpy site, have students run the following:\n",
    "\n",
    "```python\n",
    "_ = ok.submit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Example\n",
    "\n",
    "If we wanted to use okpy in this notebook, we could test that the squared durations were stored correctly. This test is stored as `tests/q2.py`, so we could check this using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = ok.grade(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Data\n",
    "\n",
    "If you make some modifications to the data set or do some data cleaning, you may want to export your data from Python to make it easier to pick up later or to reproduce. For this reason, there is a `datascience` function that allow you to export a Table object to a text file, which you can then load back into Python later. To export as a CSV file, you pass the file name (or file location, if it's going to another folder) to the `.to_csv()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.to_csv('export/trips-export.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save as another file format (e.g. TSV, JSON), you will have to export through `pandas` by setting the `sep` argument of the `.to_csv()` method or using a different export function (e.g. `pd.to_json()`). This is easily accomplished if you have a Table by transferring that table to `pandas` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transfer to pandas, from above\n",
    "trips_df = trips.to_df()\n",
    "\n",
    "# export as tsv\n",
    "trips_df.to_csv('data/trips.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook should have given you a good introduction to the `Table` class of `datascience`. This demo is not an exhaustive one, and there are _many_ other functionalities of the class that were not covered. To see these and the other functions in the library (including plotting and mapping functionality), see the [`datascience` documentation](http://data8.org/datascience)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
